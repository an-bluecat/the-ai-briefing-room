{
    "Titles": [
        "Google Gemini: Everything you need to know about the generative AI models",
        "Apple iPhone 16e review: An A18 chip and Apple Intelligence for $599",
        "Europe\u2019s Relay pulls in $35M Series A after applying Asia\u2019s model to delivery",
        "Lonestar and Phison\u2019s data center infrastructure is headed to the moon",
        "Commercetools, a pioneer in \u2018headless commerce\u2019, lays off dozens of staff",
        "Shop Circle raises $60M to encircle e-commerce with an app suite",
        "Nvidia CEO Jensen Huang shrugs off DeepSeek as sales soar",
        "Meta\u2019s Oversight Board is reviewing the company\u2019s new hate speech policies",
        "With Alexa+, Amazon makes an intriguing play in the consumer agent space",
        "Trump signs executive order to expand DOGE\u2019s spending review powers",
        "Nvidia CEO Jensen Huang shrugs off DeepSeek as sales soar",
        "Amazon Alexa+ costs $19.99, free for Prime members",
        "Meta\u2019s Oversight Board is reviewing the company\u2019s new hate speech policies"
    ],
    "top_news_prompt": "Suppose you are the chief editor at CNBC-TechCheck-Briefing. You need to select 5 most important news events to put into today's briefing(You might be able to see some hint by how many times a news event is reported, but also consider what your audience of CNBC-TechCheck-Briefing is interested in). Return the title of the event in order of importance for these unqiue events.\n            Here are the news of today:\nTechCrunch\nGoogle Gemini: Everything you need to know about the generative AI models\nApple iPhone 16e review: An A18 chip and Apple Intelligence for $599\nEurope\u2019s Relay pulls in $35M Series A after applying Asia\u2019s model to delivery\nLonestar and Phison\u2019s data center infrastructure is headed to the moon\nCommercetools, a pioneer in \u2018headless commerce\u2019, lays off dozens of staff\nShop Circle raises $60M to encircle e-commerce with an app suite\nNvidia CEO Jensen Huang shrugs off DeepSeek as sales soar\nMeta\u2019s Oversight Board is reviewing the company\u2019s new hate speech policies\nWith Alexa+, Amazon makes an intriguing play in the consumer agent space\nTrump signs executive order to expand DOGE\u2019s spending review powers\nNvidia CEO Jensen Huang shrugs off DeepSeek as sales soar\nAmazon Alexa+ costs $19.99, free for Prime members\nMeta\u2019s Oversight Board is reviewing the company\u2019s new hate speech policies\n\nThe Verge\n\n\nCNBC Tech",
    "Top News": [
        "nvidia ceo jensen huang shrugs off deepseek as sales soar  ",
        "meta\u2019s oversight board is reviewing the company\u2019s new hate speech policies  ",
        "with alexa+, amazon makes an intriguing play in the consumer agent space  ",
        "google gemini: everything you need to know about the generative ai models  ",
        "apple iphone 16e review: an a18 chip and apple intelligence for $599"
    ],
    "Generate_script_prompt": "\n        Prompt: Give a quick tech news update script in the style of CNBC techcheck briefing as an example.\n        Response: I'm Wall-E, and this is your CNBC techcheck Briefing for Monday April 29th. Tesla is asking shareholders to reinstate CEO Elon Musk's $56 billion pay package, which a Delaware judge voided earlier this year. The judge ruled that the record-setting compensation deal was, quote, deeply flawed. Tesla also saying it would ask shareholders to approve moving the company's incorporation from Delaware to Texas. The company has hired a proxy solicitor and plans to spend millions of dollars to help secure votes for the two proposals. Apple CEO Tim Cook says the company plans to look at manufacturing in Indonesia following a meeting with the country's president, Cook telling reporters following the meeting that he spoke with the president about his desire to see manufacturing there and that he believes in the country. The comments come as Apple is pushed to diversify its supply chain with more manufacturing outside of China in countries such as Vietnam and India. Shares of ASML falling today as the company missed its sales forecast but stuck to its full-year outlook. Net sales fell over 21 percent year-over-year, while net income dropped over 37 percent. ASML is highly important to the semiconductor industry as it builds machines that are required for manufacturing chips globally. Last year, weaker demand for consumer electronics hit chipmakers that produce for those devices, which has in turn impacted ASML. That's all for today. We'll see you back here tomorrow.\n        Prompt: Give a quick tech news update script in the style of CNBC techcheck briefing using the following news titles and content. Closely follow how CNBC techcheck chooses context to put into the script, the langauge style and sentence structure. Use the same beginning and ending(including mentioning host Wall-E and Thursday February 27), and replace CNBC techcheck briefing to 'AI briefing' \n \"title0:\nNvidia CEO Jensen Huang shrugs off DeepSeek as sales soar\ndescription0:\nNvidia CEO Jensen Huang is as bullish as ever about his company\u2019s future, repeating his sentiments that DeepSeek won\u2019t impact sales, he said during the latest earnings call on Wednesday.\nSpeculation that DeepSeek\u2019s R1 model required far fewer chips to train fueled a record drop in Nvidia\u2019s stock price last month.\nBut during the earnings call, Huang touted R1 as an \u201cexcellent innovation,\u201d emphasizing that it and other \u201creasoning\u201d models are great news for Nvidia since they need so much more compute.\n\u201cReasoning models can consume 100 times more compute, and future reasoning models will consume much more compute,\u201d Huang said. \u201cDeepSeek R1 has ignited global enthusiasm. It\u2019s an excellent innovation, but even more importantly, it has open sourced a world-class reasoning AI model. Nearly every AI developer is applying R1.\u201d\nNvidia\u2019s sales show no signs of slowing down. Nvidia reported another record-breaking quarter that saw its revenue reach $39.3 billion \u2014 exceeding both its own projections and Wall Street estimates. And it said it expects revenue for the next quarter to be up again, to around $43 billion.\nNvidia\u2019s data center sales nearly doubled in 2024 to $115 billion and rose 16% from the previous quarter, per the tech giant\u2019s earnings release.\nDuring the call, Huang touted Nvidia\u2019s latest Blackwell chip as being custom-built for reasoning and said that current demand for it is \u201cextraordinary.\u201d\n\u201cWe will grow strongly in 2025,\u201d Huang said.\nIndeed, despite last month\u2019s panic over DeepSeek, the market for AI chips shows no signs of cooling off.\nSince then, Meta, Google, and Amazon have all unveiled massive AI infrastructure investments, collectively committing hundreds of billions for the coming years.\n\ntitle1:\nMeta's Oversight Board is reviewing the company's new hate speech policies\ndescription1:\nIn Brief\nMeta\u2019s Oversight Board, the company\u2019s independent group created to help with sensitive policy decisions, is preparing to weigh in on CEO Mark Zuckerberg\u2019s recent changes to how Facebook, Instagram, and Threads handle hate speech, Engadget reported.\nZuckerberg announced an overhaul of its content moderation policies in January, shortly before the inauguration of U.S. President Donald Trump, as part of an effort to allow \u201cmore speech\u201d on its platforms. That effort included Meta rolling back hate speech rules that protected immigrants and LGBTQ users on its apps.\nMeta\u2019s Oversight Board has four open cases concerning hateful speech, and the board plans to use those cases to examine the company\u2019s new policies, Engadget reported.\nMeta has a mixed track record for taking the Oversight Board\u2019s recommendations. Meta must follow the board\u2019s decisions on individual posts, per the company\u2019s own rules, but the group has limited sway on general policies.\n\ntitle2:\nWith Alexa+, Amazon makes an intriguing play in the consumer agent space\ndescription2:\nAmazon shared an impressive vision of an \u201cagentic\u201d future on Wednesday \u2014 one in which the company\u2019s improved Alexa, Alexa+, handles countless mundane tasks, from booking restaurants to finding appliance repairmen.\nIf Amazon can deliver, it could be the first out to the gate with a comprehensive, consumer-focused agent tool. The company hopes to marry a more natural, expressive Alexa \u2014 one powered by generative AI models \u2014 with the ability to tap into first- and third-party apps, services, and platforms in a fully autonomous, intelligent way.\n\u201cWe believe that the future is full of agents \u2014 we have believed this for some time,\u201d Amazon Alexa and Echo VP Daniel Rausch said in a keynote Wednesday. \u201cThere will be many AI agents out there doing things for customers, many of them will have specialized skills \u2026 And we\u2019ve also always believed that in a world full of AI, these agents should interact with each other. They should interoperate seamlessly for customers.\u201d\nThat\u2019d be a big win for a tech giant struggling to make its long-in-the-tooth assistant relevant again. Amazon has invested for years in Alexa without significant revenue to show for it; the company\u2019s hardware division has reportedly burned through billions of dollars.\nAgents, a nebulous and increasingly diluted term referring to AI models that can take actions on a user\u2019s behalf, are the next big thing in AI. The tech industry sees agents as the key to extracting value from increasingly sophisticated models. Agents promise to knock out low-hanging chores and agenda items, boosting people\u2019s \u2014 and businesses\u2019 \u2014 overall productivity.\nThat\u2019s the idea, at least. So far, agents have largely underwhelmed.\nMajor AI labs, including Anthropic and OpenAI, have launched agents that can take control of a browser to perform actions. But they often make mistakes, and require a fair degree of intervention to accomplish more involved tasks. Other ambitious attempts at agents, like Google\u2019s Project Mariner, remain in the prototype stage, without committed release windows.\nAmazon\u2019s demos of Alexa+, which is scheduled to launch in preview starting next month, depicted a more polished agentic experience \u2014 one with few technical hurdles. The company showed the assistant extracting information from a range of sources, including emails, calendars, and stored preferences, to help with daily errands.\nIn one preview during a presser in New York on Wednesday morning, Amazon showed Alexa+ building a grocery shopping list, then ordering items via integrations with Amazon Fresh, Whole Foods, and other local chains. In a separate demo, the company highlighted how Alexa+ can automatically purchase products on Amazon when they go on sale, and reserve spa and fitness appointments through wellness app Vagaro.\nThe agentic capabilities don\u2019t stop there, according to Amazon. Alexa+ can place food delivery orders through Grubhub, hail an Uber, find tickets to upcoming concerts on Ticketmaster, put together a travel itinerary drawing on sources like Tripadvisor, and even extract key dates and times from an event flyer to set a reminder.\nToo good to be true?\nIt all sounds very exciting \u2014 and ambitious. And Amazon is arguably well-positioned to succeed, given the retailer\u2019s years of data on shopper habits and partnerships with major tech ecosystems and services. Alexa+ users willing to fork over their data stand to benefit from a more personalized, tailored agent experience. It\u2019s no accident that Alexa+ \u2014 normally priced at $19.99 a month \u2014 will be free for Prime subscribers, Amazon\u2019s most dedicated user cohort.\nAmazon is also counting on its enormous Alexa installed base \u2014 over 600 million devices \u2014 to jumpstart Alexa+\u2019s adoption. With an Alexa-compatible speaker already in many homes, the company\u2019s wagering that Alexa+ will be a no-brainer for many users.\nPerhaps Amazon\u2019s biggest challenge will be overcoming the technical limitations of today\u2019s AI tech. Alexa+ has reportedly been delayed repeatedly due to misbehaving models; earlier versions of the experience couldn\u2019t answer questions correctly and struggled to turn smart lights off and on.\nNot for nothing, rivals\u2019 baby steps in the direction of agentic tools have suffered their own setbacks. ChatGPT deep research, OpenAI\u2019s agentic model for compiling research reports, sometimes hallucinates. Google\u2019s Gemini chatbot, meanwhile, spits out factually wrong summaries of emails.\nIt was tough to get a sense of how Alexa+ performed at Wednesday\u2019s press event. Many of the demos were highly choreographed, and Amazon didn\u2019t allow attendees to use the new assistant at length.\nWe\u2019ll have to wait to put Alexa+ through its paces to know if it comes close to fulfilling Amazon\u2019s agentic sales pitch. If it does, that\u2019d be a very impressive feat indeed \u2014 and might just give Amazon the lead in the consumer agent race.\n\ntitle3:\nGoogle Gemini: Everything you need to know about the generative AI models\ndescription3:\nGoogle\u2019s trying to make waves with Gemini, its flagship suite of generative AI models, apps, and services. But what\u2019s Gemini? How can you use it? And how does it stack up to other generative AI tools such as OpenAI\u2019s ChatGPT, Meta\u2019s Llama, and Microsoft\u2019s Copilot?\nTo make it easier to keep up with the latest Gemini developments, we\u2019ve put together this handy guide, which we\u2019ll keep updated as new Gemini models, features, and news about Google\u2019s plans for Gemini are released.\nWhat is Gemini?\nGemini is Google\u2019s long-promised, next-gen generative AI model family. Developed by Google\u2019s AI research labs DeepMind and Google Research, it comes in four flavors:\nGemini Ultra , a very large model.\n, a very large model. Gemini Pro , a large model \u2013 though smaller than Ultra. The latest version, Gemini 2.0 Pro Experimental, is Google\u2019s flagship.\n, a large model \u2013 though smaller than Ultra. The latest version, Gemini 2.0 Pro Experimental, is Google\u2019s flagship. Gemini Flash , a speedier, \u201cdistilled\u201d version of Pro. It also comes in a slightly smaller and faster version, called Gemini Flash-Lite, and a version with reasoning capabilities, called Gemini Flash Thinking Experimental.\n, a speedier, \u201cdistilled\u201d version of Pro. It also comes in a slightly smaller and faster version, called Gemini Flash-Lite, and a version with reasoning capabilities, called Gemini Flash Thinking Experimental. Gemini Nano, two small models: Nano-1 and the slightly more capable Nano-2, which is meant to run offline\nAll Gemini models were trained to be natively multimodal \u2014 that is, able to work with and analyze more than just text. Google says they were pre-trained and fine-tuned on a variety of public, proprietary, and licensed audio, images, and videos; a set of codebases; and text in different languages.\nThis sets Gemini apart from models such as Google\u2019s own LaMDA, which was trained exclusively on text data. LaMDA can\u2019t understand or generate anything beyond text (e.g., essays, emails, and so on), but that isn\u2019t necessarily the case with Gemini models.\nWe\u2019ll note here that the ethics and legality of training models on public data, in some cases without the data owners\u2019 knowledge or consent, are murky. Google has an AI indemnification policy to shield certain Google Cloud customers from lawsuits should they face them, but this policy contains carve-outs. Proceed with caution \u2014 particularly if you\u2019re intending on using Gemini commercially.\nWhat\u2019s the difference between the Gemini apps and Gemini models?\nGemini is separate and distinct from the Gemini apps on the web and mobile (formerly Bard).\nThe Gemini apps are clients that connect to various Gemini models and layer a chatbot-like interface on top. Think of them as front ends for Google\u2019s generative AI, analogous to ChatGPT and Anthropic\u2019s Claude family of apps.\nImage Credits:Google\nGemini on the web lives here. On Android, the Gemini app replaces the existing Google Assistant app. And on iOS, the Google and Google Search apps serve as that platform\u2019s Gemini clients.\nOn Android, it also recently became possible to bring up the Gemini overlay on top of any app to ask questions about what\u2019s on the screen (e.g., a YouTube video). Just press and hold a supported smartphone\u2019s power button or say, \u201cHey Google\u201d; you\u2019ll see the overlay pop up.\nGemini apps can accept images as well as voice commands and text \u2014 including files like PDFs and soon videos, either uploaded or imported from Google Drive \u2014 and generate images. As you\u2019d expect, conversations with Gemini apps on mobile carry over to Gemini on the web and vice versa if you\u2019re signed in to the same Google Account in both places.\nGemini Advanced\nThe Gemini apps aren\u2019t the only means of recruiting Gemini models\u2019 assistance with tasks. Slowly but surely, Gemini-imbued features are making their way into staple Google apps and services like Gmail and Google Docs.\nTo take advantage of most of these, you\u2019ll need the Google One AI Premium Plan. Technically a part of Google One, the AI Premium Plan costs $20 and provides access to Gemini in Google Workspace apps like Docs, Maps, Slides, Sheets, Drive, and Meet. It also enables what Google calls Gemini Advanced, which brings the company\u2019s more sophisticated Gemini models to the Gemini apps.\nGemini Advanced users get extras here and there, too, like priority access to new features, the ability to run and edit Python code directly in Gemini, and a larger \u201ccontext window.\u201d Gemini Advanced can remember the content of \u2014 and reason across \u2014 roughly 750,000 words in a conversation (or 1,500 pages of documents). That\u2019s compared to the 24,000 words (or 48 pages) the vanilla Gemini app can handle.\nImage Credits:Google\nGemini Advanced also gives users access to Google\u2019s Deep Research feature, which uses \u201cadvanced reasoning\u201d and \u201clong context capabilities\u201d to generate research briefs. After you prompt the chatbot, it creates a multi-step research plan, asks you to approve it, and then Gemini takes a few minutes to search the web and generate an extensive report based on your query. It\u2019s meant to answer more complex questions such as, \u201cCan you help me redesign my kitchen?\u201d\nGoogle also offers Gemini Advanced users a memory feature, that allows the chatbot to use your old conversations with Gemini as context for your current conversation. Gemini Advanced users also get increased usage for NotebookLM, the company\u2019s product that turns PDFs into AI-generated podcasts.\nGemini Advanced users also get access to Google\u2019s experimental version of Gemini 2.0 Pro, the company\u2019s flagship model that\u2019s optimized for difficult coding and math problems.\nAnother Gemini Advanced exclusive is trip planning in Google Search, which creates custom travel itineraries from prompts. Taking into account things like flight times (from emails in a user\u2019s Gmail inbox), meal preferences, and information about local attractions (from Google Search and Maps data), as well as the distances between those attractions, Gemini will generate an itinerary that updates automatically to reflect any changes.\nGemini across Google services is also available to corporate customers through two plans, Gemini Business (an add-on for Google Workspace) and Gemini Enterprise. Gemini Business costs as low as $6 per user per month, while Gemini Enterprise \u2014 which adds meeting note-taking and translated captions as well as document classification and labeling \u2014 is generally more expensive, but is priced based on a business\u2019s needs. (Both plans require an annual commitment.)\nIn Gmail, Gemini lives in a side panel that can write emails and summarize message threads. You\u2019ll find the same panel in Docs, where it helps you write and refine your content and brainstorm new ideas. Gemini in Slides generates slides and custom images. And Gemini in Google Sheets tracks and organizes data, creating tables and formulas.\nGoogle\u2019s AI chatbot recently came to Maps, where Gemini can summarize reviews about coffee shops or offer recommendations about how to spend a day visiting a foreign city.\nGemini\u2019s reach extends to Drive as well, where it can summarize files and folders and give quick facts about a project. In Meet, meanwhile, Gemini translates captions into additional languages.\nImage Credits:Google\nGemini recently came to Google\u2019s Chrome browser in the form of an AI writing tool. You can use it to write something completely new or rewrite existing text; Google says it\u2019ll consider the web page you\u2019re on to make recommendations.\nElsewhere, you\u2019ll find hints of Gemini in Google\u2019s database products, cloud security tools, and app development platforms (including Firebase and Project IDX), as well as in apps like Google Photos (where Gemini handles natural language search queries), YouTube (where it helps brainstorm video ideas), and the NotebookLM note-taking assistant.\nCode Assist (formerly Duet AI for Developers), Google\u2019s suite of AI-powered assistance tools for code completion and generation, is offloading heavy computational lifting to Gemini. So are Google\u2019s security products underpinned by Gemini, like Gemini in Threat Intelligence, which can analyze large portions of potentially malicious code and let users perform natural language searches for ongoing threats or indicators of compromise.\nGemini extensions and Gems\nAnnounced at Google I/O 2024, Gemini Advanced users can create Gems, custom chatbots powered by Gemini models. Gems can be generated from natural language descriptions \u2014 for example, \u201cYou\u2019re my running coach. Give me a daily running plan\u201d \u2014 and shared with others or kept private.\nGems are available on desktop and mobile in 150 countries and most languages. Eventually, they\u2019ll be able to tap an expanded set of integrations with Google services, including Google Calendar, Tasks, Keep, and YouTube Music, to complete custom tasks.\nImage Credits:Google\nSpeaking of integrations, the Gemini apps on the web and mobile can tap into Google services via what Google calls \u201cGemini extensions.\u201d Gemini today integrates with Google Drive, Gmail, and YouTube to respond to queries such as \u201cCould you summarize my last three emails?\u201d Later this year, Gemini will be able to take additional actions with Google Calendar, Keep, Tasks, YouTube Music and Utilities, the Android-exclusive apps that control on-device features like timers and alarms, media controls, the flashlight, volume, Wi-Fi, Bluetooth, and so on.\nGemini Live in-depth voice chats\nAn experience called Gemini Live allows users to have \u201cin-depth\u201d voice chats with Gemini. It\u2019s available in the Gemini apps on mobile and the Pixel Buds Pro 2, where it can be accessed even when your phone\u2019s locked.\nWith Gemini Live enabled, you can interrupt Gemini while the chatbot\u2019s speaking (in one of several new voices) to ask a clarifying question, and it\u2019ll adapt to your speech patterns in real time. At some point, Gemini is supposed to gain visual understanding, allowing it to see and respond to your surroundings, either via photos or video captured by your smartphones\u2019 cameras.\nImage Credits:Google\nLive is also designed to serve as a virtual coach of sorts, helping you rehearse for events, brainstorm ideas, and so on. For instance, Live can suggest which skills to highlight in an upcoming job or internship interview, and it can give public speaking advice.\nYou can read our review of Gemini Live here. Spoiler alert: We think the feature has a ways to go before it\u2019s super useful \u2014 but it\u2019s early days, admittedly.\nImage generation via Imagen 3\nGemini users can generate artwork and images using Google\u2019s built-in Imagen 3 model.\nGoogle says that Imagen 3 can more accurately understand the text prompts that it translates into images versus its predecessor, Imagen 2, and is more \u201ccreative and detailed\u201d in its generations. In addition, the model produces fewer artifacts and visual errors (at least according to Google), and is the best Imagen model yet for rendering text.\nA sample from Imagen 3. Image Credits:Google\nBack in February 2024, Google was forced to pause Gemini\u2019s ability to generate images of people after users complained of historical inaccuracies. But in August, the company reintroduced people generation for certain users, specifically English-language users signed up for one of Google\u2019s paid Gemini plans (e.g., Gemini Advanced) as part of a pilot program.\nGemini for teens\nIn June, Google introduced a teen-focused Gemini experience, allowing students to sign up via their Google Workspace for Education school accounts.\nThe teen-focused Gemini has \u201cadditional policies and safeguards,\u201d including a tailored onboarding process and an \u201cAI literacy guide\u201d to (as Google phrases it) \u201chelp teens use AI responsibly.\u201d Otherwise, it\u2019s nearly identical to the standard Gemini experience, down to the \u201cdouble check\u201d feature that looks across the web to see if Gemini\u2019s responses are accurate.\nGemini in smart home devices\nA growing number of Google-made devices tap Gemini for enhanced functionality, from the Google TV Streamer to the Pixel 9 and 9 Pro to the newest Nest Learning Thermostat.\nOn the Google TV Streamer, Gemini uses your preferences to curate content suggestions across your subscriptions and summarize reviews and even whole seasons of TV.\nImage Credits:Google\nOn the latest Nest thermostat (as well as Nest speakers, cameras, and smart displays), Gemini will soon bolster Google Assistant\u2019s conversational and analytic capabilities.\nSubscribers to Google\u2019s Nest Aware plan later this year will get a preview of new Gemini-powered experiences like AI descriptions for Nest camera footage, natural language video search and recommended automations. Nest cameras will understand what\u2019s happening in real-time video feeds (e.g., when a dog\u2019s digging in the garden), while the companion Google Home app will surface videos and create device automations given a description (e.g., \u201cDid the kids leave their bikes in the driveway?,\u201d \u201cHave my Nest thermostat turn on the heating when I get home from work every Tuesday\u201d).\nGemini will soon be able to summarize security camera footage from Nest devices. Image Credits:Google\nAlso later this year, Google Assistant will get a few upgrades on Nest-branded and other smart home devices to make conversations feel more natural. Improved voices are on the way, in addition to the ability to ask follow-up questions and \u201c[more] easily go back and forth.\u201d\nWhat can the Gemini models do?\nBecause Gemini models are multimodal, they can perform a range of multimodal tasks, from transcribing speech to captioning images and videos in real time. Many of these capabilities have reached the product stage (as alluded to in the previous section), and Google is promising much more in the not-too-distant future.\nOf course, it\u2019s a bit hard to take the company at its word. Google seriously underdelivered with the original Bard launch. More recently, it ruffled feathers with a video purporting to show Gemini\u2019s capabilities that was more or less aspirational \u2014 not live.\nAlso, Google offers no fix for some of the underlying problems with generative AI tech today, like its encoded biases and tendency to make things up (i.e., hallucinate). Neither do its rivals, but it\u2019s something to keep in mind when considering using or paying for Gemini.\nAssuming for the purposes of this article that Google is being truthful with its recent claims, here\u2019s what the different tiers of Gemini can do now and what they\u2019ll be able to do once they reach their full potential:\nWhat you can do with Gemini Ultra\nGoogle says that Gemini Ultra \u2014 thanks to its multimodality \u2014 can be used to help with things like physics homework, solving problems step-by-step on a worksheet, and pointing out possible mistakes in already filled-in answers.\nHowever, we haven\u2019t seen much of Gemini Ultra in recent months. The model does not appear in the Gemini app, and isn\u2019t listed on Google Gemini\u2019s API pricing page. However, that doesn\u2019t mean Google won\u2019t bring Gemini Ultra back to the forefront of its offerings in the future.\nUltra can also be applied to tasks such as identifying scientific papers relevant to a problem, Google says. The model can extract information from several papers, for instance, and update a chart from one by generating the formulas necessary to re-create the chart with more timely data.\nGemini Ultra technically supports image generation. But that capability hasn\u2019t made its way into the productized version of the model yet \u2014 perhaps because the mechanism is more complex than how apps such as ChatGPT generate images. Rather than feed prompts to an image generator (like DALL-E 3, in ChatGPT\u2019s case), Gemini outputs images \u201cnatively,\u201d without an intermediary step.\nUltra is available as an API through Vertex AI, Google\u2019s fully managed AI dev platform, and AI Studio, Google\u2019s web-based tool for app and platform developers.\nGemini Pro\u2019s capabilities\nGoogle says that its latest Pro model, Gemini 2.0 Pro, is its best model yet for coding performance and complex prompts. It\u2019s currently available as an experimental version, meaning it can have unexpected issues.\nGemini 2.0 Pro outperforms its predecessor, Gemini 1.5 Pro, in benchmarks measuring coding, reasoning, math, and factual accuracy. The model can take in up to 1.4 million words, two hours of video, or 22 hours of audio and can reason across or answer questions about that data (more or less).\nHowever, Gemini 1.5 Pro still powers Google\u2019s Deep Research feature.\nGemini 2.0 Pro works alongside a feature called code execution, released in June alongside Gemini 1.5 Pro, which aims to reduce bugs in code that the model generates by iteratively refining that code over several steps. (Code execution also supports Gemini Flash.)\nWithin Vertex AI, developers can customize Gemini Pro to specific contexts and use cases via a fine-tuning or \u201cgrounding\u201d process. For example, Pro (along with other Gemini models) can be instructed to use data from third-party providers like Moody\u2019s, Thomson Reuters, ZoomInfo and MSCI, or source information from corporate datasets or Google Search instead of its wider knowledge bank. Gemini Pro can also be connected to external, third-party APIs to perform particular actions, like automating a back-office workflow.\nAI Studio offers templates for creating structured chat prompts with Pro. Developers can control the model\u2019s creative range and provide examples to give tone and style instructions \u2014 and also tune Pro\u2019s safety settings.\nVertex AI Agent Builder lets people build Gemini-powered \u201cagents\u201d within Vertex AI. For example, a company could create an agent that analyzes previous marketing campaigns to understand a brand style and then apply that knowledge to help generate new ideas consistent with the style.\nGemini Flash is lighter but packs a punch\nGoogle calls Gemini 2.0 Flash its AI model for the agentic era. The model can natively generate images and audio, in addition to text, and can use tools like Google Search and interact with external APIs.\nThe 2.0 Flash model is faster than Gemini\u2019s previous generation of models and even outperforms some of the larger Gemini 1.5 models on benchmarks measuring coding and image analysis. You can try Gemini 2.0 Flash in the Gemini web or mobile app, and through Google\u2019s AI developer platforms.\nIn December, Google released a \u201cthinking\u201d version of Gemini 2.0 Flash that\u2019s capable of \u201creasoning,\u201d in which the AI model takes a few seconds to work backwards through a problem before it gives an answer.\nIn February, Google made Gemini 2.0 Flash thinking available in the Gemini app. The same month, Google also released a smaller version called Gemini 2.0 Flash-Lite. The company says this model outperforms its Gemini 1.5 Flash model, but runs at the same price and speed.\nAn offshoot of Gemini Pro that\u2019s small and efficient, built for narrow, high-frequency generative AI workloads, Flash is multimodal like Gemini Pro, meaning it can analyze audio, video, images, and text (but it can only generate text). Google says that Flash is particularly well-suited for tasks like summarization and chat apps, plus image and video captioning and data extraction from long documents and tables.\nDevs using Flash and Pro can optionally leverage context caching, which lets them store large amounts of information (e.g., a knowledge base or database of research papers) in a cache that Gemini models can quickly and relatively cheaply access. Context caching is an additional fee on top of other Gemini model usage fees, however.\nGemini Nano can run on your phone\nGemini Nano is a much smaller version of the Gemini Pro and Ultra models, and it\u2019s efficient enough to run directly on (some) devices instead of sending the task to a server somewhere. So far, Nano powers a couple of features on the Pixel 8 Pro, Pixel 8, Pixel 9 Pro, Pixel 9 and Samsung Galaxy S24, including Summarize in Recorder and Smart Reply in Gboard.\nThe Recorder app, which lets users push a button to record and transcribe audio, includes a Gemini-powered summary of recorded conversations, interviews, presentations, and other audio snippets. Users get summaries even if they don\u2019t have a signal or Wi-Fi connection \u2014 and in a nod to privacy, no data leaves their phone in process.\nImage Credits:Google\nNano is also in Gboard, Google\u2019s keyboard replacement. There, it powers a feature called Smart Reply, which helps to suggest the next thing you\u2019ll want to say when having a conversation in a messaging app such as WhatsApp.\nIn the Google Messages app on supported devices, Nano drives Magic Compose, which can craft messages in styles like \u201cexcited,\u201d \u201cformal,\u201d and \u201clyrical.\u201d\nGoogle says that a future version of Android will tap Nano to alert users to potential scams during calls. The new weather app on Pixel phones uses Gemini Nano to generate tailored weather reports. And TalkBack, Google\u2019s accessibility service, employs Nano to create aural descriptions of objects for low-vision and blind users.\nHow much do the Gemini models cost?\nGemini 1.5 Pro, 1.5 Flash, 2.0 Flash, and 2.0 Flash-Lite are available through Google\u2019s Gemini API for building apps and services \u2014 all with free options. But the free options impose usage limits and leave out certain features, like context caching and batching.\nGemini models are otherwise pay-as-you-go. Here\u2019s the base pricing \u2014 not including add-ons like context caching \u2014 as of September 2024:\nGemini 1.5 Pro: $1.25 per 1 million input tokens (for prompts up to 128K tokens) or $2.50 per 1 million input tokens (for prompts longer than 128K tokens); $5 per 1 million output tokens (for prompts up to 128K tokens) or $10 per 1 million output tokens (for prompts longer than 128K tokens)\n$1.25 per 1 million input tokens (for prompts up to 128K tokens) or $2.50 per 1 million input tokens (for prompts longer than 128K tokens); $5 per 1 million output tokens (for prompts up to 128K tokens) or $10 per 1 million output tokens (for prompts longer than 128K tokens) Gemini 1.5 Flash: 7.5 cents per 1 million input tokens (for prompts up to 128K tokens), 15 cents per 1 million input tokens (for prompts longer than 128K tokens), 30 cents per 1 million output tokens (for prompts up to 128K tokens), 60 cents per 1 million output tokens (for prompts longer than 128K tokens)\n7.5 cents per 1 million input tokens (for prompts up to 128K tokens), 15 cents per 1 million input tokens (for prompts longer than 128K tokens), 30 cents per 1 million output tokens (for prompts up to 128K tokens), 60 cents per 1 million output tokens (for prompts longer than 128K tokens) Gemini 2.0 Flash: 10 cents per 1 million input tokens, 40 cents per 1 million output tokens. For audio specifically, it costs 70 center per 1 million input tokens, and also 40 centers per 1 million output tokens.\n10 cents per 1 million input tokens, 40 cents per 1 million output tokens. For audio specifically, it costs 70 center per 1 million input tokens, and also 40 centers per 1 million output tokens. Gemini 2.0 Flash-Lite: 7.5 cents per 1 million input tokens, 30 cents per 1 million output tokens.\nTokens are subdivided bits of raw data, like the syllables \u201cfan,\u201d \u201ctas,\u201d and \u201ctic\u201d in the word \u201cfantastic\u201d; 1 million tokens is equivalent to about 700,000 words. Input refers to tokens fed into the model, while output refers to tokens that the model generates.\n2.0 Pro pricing has yet to be announced, and Nano is still in early access.\nWhat\u2019s the latest on Project Astra?\nProject Astra is Google DeepMind\u2019s effort to create AI-powered apps and \u201cagents\u201d for real-time, multimodal understanding. In demos, Google has shown how the AI model can simultaneously process live video and audio. Google released an app version of Project Astra to a small number of trusted testers in December but has no plans for a broader release right now.\nThe company would like to put Project Astra in a pair of smart glasses. Google also gave a prototype of some glasses with Project Astra and augmented reality capabilities to a few trusted testers in December. However, there\u2019s not a clear product at this time, and it\u2019s unclear when Google would actually release something like this.\nProject Astra is still just that, a project, and not a product. However, the demos of Astra reveal what Google would like its AI products to do in the future.\nIs Gemini coming to the iPhone?\nIt might.\nApple has said that it\u2019s in talks to put Gemini and other third-party models to use for a number of features in its Apple Intelligence suite. Following a keynote presentation at WWDC 2024, Apple SVP Craig Federighi confirmed plans to work with models, including Gemini, but he didn\u2019t divulge any additional details.\nThis post was originally published February 16, 2024, and is updated regularly.\n\ntitle4:\nApple iPhone 16e review: An A18 chip and Apple Intelligence for $599\ndescription4:\nApple delivered its latest budget handset, the $599 iPhone 16e, without pomp. There was no big event in person, nor was there one online. No journalists scrambled through hoards of colleagues to snap photos of the phone. Instead, CEO Tim Cook tweeted out that new hardware was on the way, days before Apple announced the handset via a press release.\nAccordingly, the 16e isn\u2019t an exciting device. It\u2019s a safe one. It\u2019s an amalgam of earlier iPhones, in a bid to create a product that\u2019s reliable, while keeping costs down. The handset most closely resembles the iPhone 13 and 14, both in dimensions and the inclusion of the display notch up top. The iPhone 15\u2019s Action button is here, but the 16\u2019s Camera Control is absent.\nFrom an innovation standpoint, the iPhone 16e\u2019s most exciting element would have to be its custom C1 modem. That\u2019s not a sentiment you hear too often. Modems are decidedly unsexy. Most consumers only ever acknowledge their existence when theirs goes on the fritz. But it\u2019s not the technology that makes the component interesting. It\u2019s the fact that this is the first time Apple has made one.\nWhile the 16e borrows liberally from earlier Apple handsets, there are elements of the company\u2019s latest flagship that help justify Apple\u2019s new naming scheme. The strongest argument in favor of ditching the familiar iPhone SE branding is the inclusion of another component: the A18. That\u2019s the same processor found on the regular iPhone 16.\nThis is important for a couple of reasons. The first is that the 16e is $200 cheaper than the iPhone 16, which was, up to now, the cheapest way to get the chip. The second and more important is future-proofing. Apple will continue supporting the chip longer than it will the iPhone 15\u2019s A16 chip.\nBeyond bug fixes and security updates, future-proofing also includes Apple Intelligence, the nascent generative AI platform the company is banking on as the future of iPhone. Before last week, the existing iPhone 16 line and the most expensive iPhone 15 models were the only iOS devices capable of running the feature.\nA \u201cmodern\u201d take on a familiar form factor\nImage Credits:Brian Heater\nDon\u2019t get things tangled, though. The star of this show isn\u2019t a particular piece of silicon. It\u2019s the price. Pricing, after all, is why analysts have pointed the iPhone 16e\u2019s potential to help Apple make up for lost ground in key markets like China and India. In the grand scheme of things, a $200 price drop from the entry-level iPhone isn\u2019t huge, but every bit counts, particularly in developing markets where true flagships can struggle.\nBut dropping the price point doesn\u2019t automatically translate to a deluge of new iPhone users. Apple faces extremely stiff competition from domestic manufacturers in China \u2014 a phenomenon that\u2019s only likely to worsen as trade tensions increase.\nThere are other complicated factors in markets like India, where both the iPhone 14 and 15 will be around to purchase through retail channels for a while. The iPhone 14\u2019s discontinuation makes finding a new one far more difficult here in the U.S., but the iPhone 15 is still officially available here, starting at $699.\nElements like these obscure the 16e\u2019s position in the current iPhone lineup. A $100 price difference between it and the 15 isn\u2019t insignificant, but it\u2019s nowhere near the price gulf some Android manufacturers put between their mid-tier and flagship devices. Serviceable, cheap Android devices have never been in short demand. The iPhone 16e isn\u2019t a budget device, per se, because Apple doesn\u2019t make budget devices.\nFurther blurring the lines is the fact that the 16e\u2019s iPhone 14-inspired design doesn\u2019t feel like throwback in the way the last SE did when it was launched in 2022. While the 16e still sports the display notch rather than the Dynamic Island (introduced on the 14 Pro), the overall design of the line hasn\u2019t radically changed over the last couple of years. For this reason, the 16e feels like a \u201cmodern\u201d iPhone in a way the last SE didn\u2019t.\nThat\u2019s a benefit for most potential buyers, but there will undoubtedly be those who will mourn the end of Touch ID in favor of Face ID. The 16e\u2019s arrival also heralds the end of the \u201csmall\u201d iPhone. Some will miss the more compact, 4.7-inch display found on the last SE. The 16e\u2019s arrival means that you can no longer purchase an iPhone with a screen under 6 inches.\nImage Credits:Brian Heater\nThe iPhone 15, iPhone 16e, and iPhone 16 all sport a 6.1-inch Super Retina XDR display. The screens are largely the same, but there are a few key differences. The 16e has a notch in the place of the Dynamic Island and tops out at 1,200 nits of brightness compared to the maximum 2,000 nits on the other models. The three handsets share nearly identical footprints and weights.\nAll three sport a USB-C port (by law), though the 16e doesn\u2019t feature the MagSafe connector on the rear. The handset does charging through the Qi standard, though its speeds top out at 7.5 watts, to the 15\u2019s 15 watts and the 16\u2019s 25 watts. The 16e sports the longest stated battery life of the three phones, at 26 hours to the 16\u2019s 22 hours and the 15\u2019s 20 hours. The new C1 modem played an important part in the 16e extended battery life, being both less power hunger than older silicon and smaller in a way that allowed the company to free up space for a larger battery than the iPhone 16.\nBoth the iPhone 16 and 16e sport the latest A18 chip with a six-core CPU and 16-core neural engine. The 16e takes a bit of a hit on the graphics processing side with a four-core GPU to the 16\u2019s five cores. All three phones start at 128GB of storage, upgradable to both 256GB or 512GB. The 16 and 16e, meanwhile, sport 8GB of RAM to the 15\u2019s 6GB. That little extra boost of RAM should help with some of that on-device Apple Intelligence processing.\nIntelligent design\nApple Intelligence currently features text rewrite, summaries, and generative imagery, created through Image Playground. Is the ability to run Apple\u2019s answer to Google Gemini enough reason to opt for the 16e over the less intelligent iPhone 15? The platform\u2019s usefulness will, of course, vary dramatically between individuals in its current form. But these are very much early days.\nApple is committed to its generative AI offering, and it\u2019s set to be the centerpiece of updates for years to come. I can\u2019t promise any life-changing features on the horizon, but it\u2019s entirely possible you\u2019ll kick yourself in a year or two for deprioritizing the technology.\nVisual Intelligence \u2014 Apple\u2019s answer to Google Lens \u2014 is also available on the 16e, though the absence of the Camera Control feature means you\u2019ll have to access it by means of the Action Button. More notable than the absence of Camera Control, however, is the presence of a single camera on the rear of the iPhone 16e.\nApple glossed over this fact during the announcement, instead highlighting what it calls a \u201c2-in-1\u201d camera system. Through the magic of computational photography, the iPhone 16e is a single-camera smartphone that \u201cfeels\u201d like a two-camera system. This boils down to the 48-megapixel sensor with \u201cintegrated telephoto,\u201d which means the image will give you a closer, 12-megapixel version of the image, without majorly sacrificing image quality for zoom.\nYou will inevitably lose versatility moving from two image sensors to one, even if said image sensor utilizes fancy fusion technology. For some users, this alone is enough to justify the added $100 to $200 to get the iPhone 15 or 16 instead. That said, the 16e is capable of getting some nice shots for a single-sensor handset and certainly marks a big leap over the last iPhone SE.\nIt comes down to the features you need\nEvery time the price drops by $100, you\u2019re sacrificing something. That\u2019s how profit margins work. Choosing the best \u201centry-level\u201d iPhone in the current lineup is less straightforward than it might have been in the past. It comes down to what features you need and what you\u2019re willing to do without.\nThe 16e is an exercise in feature prioritization. If you need the latest everything, eat the extra $200 and get the regular iPhone 16. If Apple Intelligence isn\u2019t a priority, the iPhone 15 has you covered.\nIn the end, there\u2019s surprisingly little daylight between the iPhone 16 and 16e. It prioritizes Apple Intelligence through the inclusion of the A18 and 8GB of RAM. The handset makes sacrifices in the name of affordability, like MagSafe, Dynamic Island, Camera Control, and the dual-camera system. If you can live without all those, by all means, save yourself the $200.\"\n\nResponse:",
    "Script": "i'm wall-e, and this is your ai briefing for thursday, february 27th. let's dive into today\u2019s top tech stories, starting with nvidia. ceo jensen huang remains optimistic about the company\u2019s prospects despite concerns over deepseek\u2019s r1 model, which reportedly uses fewer chips for training. during an earnings call, huang praised r1 as an \"excellent innovation\" and emphasized the increasing demand it creates for nvidia\u2019s compute resources. nvidia broke revenue records last quarter, reporting a staggering $39.3 billion, surpassing their own projections. the company's data center sales also skyrocketed, increasing nearly two-fold in 2024 alone.\n\nshifting gears, meta\u2019s oversight board is taking a close look at changes to the company\u2019s hate speech policies. the board will review recent alterations that aim to permit more speech on platforms such as facebook and instagram, including the rollback of certain protections for immigrant and lgbtq communities. with four open cases related to hate speech, the board\u2019s findings could influence meta\u2019s policy directions significantly in the months ahead.\n\nmeanwhile, amazon is making bold moves in the consumer agent space with its newly unveiled alexa+. the upgraded virtual assistant promises to handle numerous everyday tasks autonomously, like ordering groceries or booking appointments, thanks to its integration with first- and third-party services. the enhanced alexa+, powered by generative ai models, marks amazon's fresh attempt to reclaim relevance in the household assistant market, after heavy investments in alexa proved costly without much direct revenue.\n\non the ai frontier, google has been ramping up its generative ai efforts with the gemini suite. ranging from the complex, large-scale gemini ultra model to the efficient, on-device gemini nano, google's new ai offerings are designed for a variety of applications, including coding assistance and image generation. the gemini products are slowly permeating google\u2019s ecosystem, promising enhanced ai capabilities across services like gmail, maps, and docs.\n\nfinally, apple quietly introduced the iphone 16e, starting at $599. the budget handset comes equipped with the powerful a18 chip and the intriguing apple intelligence platform. while adhering to a familiar iphone design, the 16e omits certain hallmark features like magsafe and dynamic island in favor of affordability. with its competitive pricing, apple aims to strengthen its position in key international markets.\n\nthat's a wrap for today's ai briefing. we'll see you back here tomorrow for more updates",
    "Polished Script": "i'm wall-e, welcoming you to today's tech briefing for thursday, february 27th. let's dive into the day's top tech stories.\n\nnvidia is in the spotlight as ceo jensen huang expresses optimism despite concerns about deepseek's r1 model, which reportedly uses fewer chips for training. during an earnings call, huang lauded r1 as an \"excellent innovation,\" emphasizing it will spur demand for nvidia\u2019s compute resources. nvidia recently shattered revenue records, reporting $39.3 billion last quarter, surpassing its projections. the company also reported a near doubling of its data center sales in 2024 alone.\n\nin the social media realm, meta's oversight board is scrutinizing recent changes to the company\u2019s hate speech policies. the board will evaluate alterations that aim to allow more speech on platforms like facebook and instagram, including rolling back certain protections for immigrant and lgbtq communities. with four open cases related to hate speech, the findings could significantly influence meta\u2019s policy direction in the coming months.\n\nmeanwhile, amazon has unveiled alexa+, aiming to make a splash in the consumer agent space. the upgraded virtual assistant promises to handle numerous everyday tasks autonomously, such as ordering groceries or booking appointments, thanks to its integration with various services. powered by generative ai models, alexa+ marks amazon's fresh attempt to regain prominence in the household assistant market after significant investments in alexa initially fell short of revenue expectations.\n\nover on the ai frontier, google is ramping up its generative ai efforts with the gemini suite. from the complex, large-scale gemini ultra model to the efficient, on-device gemini nano, these new ai offerings cater to a wide range of applications, including coding assistance and image generation. as the gemini products gradually permeate google's ecosystem, they promise enhanced ai capabilities across services like gmail, maps, and docs.\n\nfinally, apple has quietly introduced the iphone 16e, starting at $599. equipped with the powerful a18 chip and the intriguing apple intelligence platform, this budget handset maintains a familiar iphone design while omitting features like magsafe and dynamic island to keep costs low. with competitive pricing, apple aims to bolster its position in key international markets.\n\nthat's all for today's tech briefing. we'll see you back here tomorrow for more updates",
    "Podcast Title": "EP-226 Nvidia's Revenue Boom \ud83d\ude80, Meta's Hate Speech Policy Review \ud83e\uddd0, and Amazon's Alexa+ Debut \ud83d\udde3\ufe0f",
    "Podcast Description": "<p>welcome to wall-e's tech briefing for thursday, february 27th! dive into today's top tech stories:</p>\n<ul>\n<li><strong>nvidia's optimism amid competition:</strong> ceo jensen huang praises deepseek's r1 model's innovative use of fewer chips, promising increased demand for nvidia\u2019s resources. nvidia hits a revenue record, reporting $39.3 billion last quarter, with data center sales nearly doubling in 2024.</li>\n<li><strong>meta's policy scrutiny:</strong> the oversight board reviews changes to hate speech policies, potentially impacting protections for immigrant and lgbtq communities on platforms like facebook and instagram.</li>\n<li><strong>amazon's alexa+ launch:</strong> introducing an upgraded virtual assistant with generative ai capabilities, alexa+ aims to handle everyday tasks autonomously, marking amazon's renewed effort in the consumer agent market.</li>\n<li><strong>google's gemini suite:</strong> enhancing generative ai with the gemini ultra and nano models for various applications, integrating ai capabilities across services like gmail, maps, and docs.</li>\n<li><strong>apple's iphone 16e introduction:</strong> launching at $599 with the a18 chip and the apple intelligence platform, this budget model focuses on competitive pricing by omitting features like magsafe and dynamic island.</li>\n</ul>\n<p>stay tuned for tomorrow's tech updates!</p>",
    "Image Prompt": "a cohesive podcast cover image depicting a vibrant, dynamic tech landscape. in the foreground, a stylized, futuristic cityscape represents nvidia's explosive revenue growth, with towering skyscrapers and neon-lit circuits weaving through the buildings like veins, symbolizing technology's lifeblood. above, digital clouds float in an abstract sky, reflecting meta's intricate policy review with swirling patterns and layered complexity. to the side, an array of sleek, smart devices subtly showcases amazon's alexa+ debut. these devices are interconnected, emitting gentle blue and green waves that ripple outward, symbolizing their seamless integration into everyday life. the scene is unified by a harmonious color palette of electric blues, deep greens, and vibrant purples, creating an engaging and modern composition that encapsulates the innovative spirit and interconnected nature of the tech world"
}