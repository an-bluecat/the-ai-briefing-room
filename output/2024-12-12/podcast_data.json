{
    "Titles": [
        "Character.AI has retrained its chatbots to stop chatting up teens",
        "Okami is getting a sequel",
        "How to watch the 2024 Game Awards",
        "FromSoftware debuts a new Elden Ring game",
        "Here\u2019s your very first look at The Witcher 4",
        "Elon Musk is mad at the SEC again",
        "Nvidia will host its first full LAN party in over a decade, and you\u2019re invited",
        "New Apple TV and HomePod mini reportedly coming in 2025",
        "Reddit bans posting UnitedHealthcare shooter\u2019s manifesto",
        "Sundar Pichai and Jeff Bezos head to Mar-a-Lago to schmooze with Trump",
        "Google says its breakthrough quantum chip can\u2019t break modern cryptography",
        "Adobe now has a tool to get rid of ugly window reflections in photos",
        "Verizon is using 5G network slicing to offer better video calling \u2014 for a price",
        "Toyota\u2019s next EV is this small Urban Cruiser SUV for Europe",
        "Amazon\u2019s Prime Video pushes \u2018AI Topics\u2019 instead of the old algorithms",
        "Meta makes $1 million donation to Trump\u2019s inauguration",
        "Xbox Game Pass Ultimate subscriptions are up to 50 percent off right now",
        "Threads follows Bluesky\u2019s starter packs with curated collections of people to follow",
        "Adobe shares suffer steepest drop in over two years on disappointing revenue guidance",
        "ServiceTitan pops to $101 in Nasdaq debut after selling shares at $71",
        "Texas House introduces bill to establish a strategic bitcoin reserve",
        "Venture capitalists bet on Sublime, a startup bringing AI to email security",
        "OpenAI says ChatGPT service has recovered after hours-long outage",
        "ServiceTitan prices IPO at $71, above expected range, after slow stretch for tech deals",
        "Microsoft debuts Phi-4, a new generative AI model, in research preview",
        "Meta debuts a tool for watermarking AI-generated videos",
        "Google Gemini: Everything you need to know about the generative AI models",
        "A Waymo robotaxi got stuck in a roundabout loop",
        "Fleet Space raises $100M to scale satellite-enabled mineral prospecting tech",
        "Carta is making it too difficult to cancel subscriptions, some founders say",
        "Tesla\u2019s loss is Zoox\u2019s gain",
        "Nearly half of US teens are online almost constantly, Pew study finds",
        "ChatGPT: Everything you need to know about the AI-powered chatbot",
        "A comprehensive list of 2024 tech layoffs",
        "ChatGPT now understands real-time video, seven months after OpenAI first demoed it",
        "A Waymo robotaxi got stuck in a roundabout loop",
        "Fleet Space raises $100M to scale satellite-enabled mineral prospecting tech",
        "Tesla\u2019s loss is Zoox\u2019s gain",
        "Nearly half of US teens are online almost constantly, Pew study finds",
        "You can make ChatGPT sound like Santa Claus for the holidays",
        "Zomato faces $95 million India tax bill"
    ],
    "top_news_prompt": "Suppose you are the chief editor at CNBC-TechCheck-Briefing. You need to select 5 most important news events to put into today's briefing(You might be able to see some hint by how many times a news event is reported, but also consider what your audience of CNBC-TechCheck-Briefing is interested in). Return the title of the event in order of importance for these unqiue events.\n            Here are the news of today:\nTechCrunch\nMicrosoft debuts Phi-4, a new generative AI model, in research preview\nMeta debuts a tool for watermarking AI-generated videos\nGoogle Gemini: Everything you need to know about the generative AI models\nA Waymo robotaxi got stuck in a roundabout loop\nFleet Space raises $100M to scale satellite-enabled mineral prospecting tech\nCarta is making it too difficult to cancel subscriptions, some founders say\nTesla\u2019s loss is Zoox\u2019s gain\nNearly half of US teens are online almost constantly, Pew study finds\nChatGPT: Everything you need to know about the AI-powered chatbot\nA comprehensive list of 2024 tech layoffs\nChatGPT now understands real-time video, seven months after OpenAI first demoed it\nA Waymo robotaxi got stuck in a roundabout loop\nFleet Space raises $100M to scale satellite-enabled mineral prospecting tech\nTesla\u2019s loss is Zoox\u2019s gain\nNearly half of US teens are online almost constantly, Pew study finds\nYou can make ChatGPT sound like Santa Claus for the holidays\nZomato faces $95 million India tax bill\n\nThe Verge\nCharacter.AI has retrained its chatbots to stop chatting up teens\nOkami is getting a sequel\nHow to watch the 2024 Game Awards\nFromSoftware debuts a new Elden Ring game\nHere\u2019s your very first look at The Witcher 4\nElon Musk is mad at the SEC again\nNvidia will host its first full LAN party in over a decade, and you\u2019re invited\nNew Apple TV and HomePod mini reportedly coming in 2025\nReddit bans posting UnitedHealthcare shooter\u2019s manifesto\nSundar Pichai and Jeff Bezos head to Mar-a-Lago to schmooze with Trump\nGoogle says its breakthrough quantum chip can\u2019t break modern cryptography\nAdobe now has a tool to get rid of ugly window reflections in photos\nVerizon is using 5G network slicing to offer better video calling \u2014 for a price\nToyota\u2019s next EV is this small Urban Cruiser SUV for Europe\nAmazon\u2019s Prime Video pushes \u2018AI Topics\u2019 instead of the old algorithms\nMeta makes $1 million donation to Trump\u2019s inauguration\nXbox Game Pass Ultimate subscriptions are up to 50 percent off right now\nThreads follows Bluesky\u2019s starter packs with curated collections of people to follow\n\nCNBC Tech\nAdobe shares suffer steepest drop in over two years on disappointing revenue guidance\nServiceTitan pops to $101 in Nasdaq debut after selling shares at $71\nTexas House introduces bill to establish a strategic bitcoin reserve\nVenture capitalists bet on Sublime, a startup bringing AI to email security\nOpenAI says ChatGPT service has recovered after hours-long outage\nServiceTitan prices IPO at $71, above expected range, after slow stretch for tech deals",
    "Top News": [
        "microsoft debuts phi-4, a new generative ai model, in research preview  ",
        "google gemini: everything you need to know about the generative ai models  ",
        "adobe shares suffer steepest drop in over two years on disappointing revenue guidance  ",
        "openai says chatgpt service has recovered after hours-long outage  ",
        "meta debuts a tool for watermarking ai-generated videos"
    ],
    "Generate_script_prompt": "\n        Prompt: Give a quick tech news update script in the style of CNBC techcheck briefing as an example.\n        Response: I'm Wall-E, and this is your CNBC techcheck Briefing for Monday April 29th. Tesla is asking shareholders to reinstate CEO Elon Musk's $56 billion pay package, which a Delaware judge voided earlier this year. The judge ruled that the record-setting compensation deal was, quote, deeply flawed. Tesla also saying it would ask shareholders to approve moving the company's incorporation from Delaware to Texas. The company has hired a proxy solicitor and plans to spend millions of dollars to help secure votes for the two proposals. Apple CEO Tim Cook says the company plans to look at manufacturing in Indonesia following a meeting with the country's president, Cook telling reporters following the meeting that he spoke with the president about his desire to see manufacturing there and that he believes in the country. The comments come as Apple is pushed to diversify its supply chain with more manufacturing outside of China in countries such as Vietnam and India. Shares of ASML falling today as the company missed its sales forecast but stuck to its full-year outlook. Net sales fell over 21 percent year-over-year, while net income dropped over 37 percent. ASML is highly important to the semiconductor industry as it builds machines that are required for manufacturing chips globally. Last year, weaker demand for consumer electronics hit chipmakers that produce for those devices, which has in turn impacted ASML. That's all for today. We'll see you back here tomorrow.\n        Prompt: Give a quick tech news update script in the style of CNBC techcheck briefing using the following news titles and content. Closely follow how CNBC techcheck chooses context to put into the script, the langauge style and sentence structure. Use the same beginning and ending(including mentioning host Wall-E and Friday December 13), and replace CNBC techcheck briefing to 'AI briefing' \n \"title0:\nMicrosoft debuts Phi-4, a new generative AI model, in research preview\ndescription0:\nMicrosoft has announced the newest addition to its Phi family of generative AI models.\nCalled Phi-4, the model is improved in several areas over its predecessors, Microsoft claims \u2014 in particular math problem solving. That\u2019s partly the result of improved training data quality.\nPhi-4 is available in very limited access as of Thursday night: only on Microsoft\u2019s recently launched Azure AI Foundry development platform, and only for research purposes under a Microsoft research license agreement.\nThis is Microsoft\u2019s latest small language model, coming in at 14 billion parameters in size, and it competes with other small models such as GPT-4o mini, Gemini 2.0 Flash, and Claude 3.5 Haiku. These AI models are oftentimes faster and cheaper to run, but the performance of small language models has gradually increased over the last several years.\nIn this case, Microsoft attributes Phi-4\u2019s jump in performance to the use of \u201chigh-quality synthetic datasets,\u201d alongside high-quality datasets of human-generated content and some unspecified post-training improvements.\nMany AI labs are looking more closely at innovations they can make around synthetic data and post training these days. Scale AI CEO Alexandr Wang said in a tweet on Thursday that \u201cwe have reached a pre-training data wall,\u201d confirming several reports on the topic in the last several weeks.\nNotably, Phi-4 is the first Phi-series model to launch following the departure of S\u00e9bastien Bubeck. Previously an AI VP at Microsoft and a key figure in the company\u2019s Phi model development, Bubeck left Microsoft in October to join OpenAI.\n\ntitle1:\nGoogle Gemini: Everything you need to know about the generative AI models\ndescription1:\nGoogle\u2019s trying to make waves with Gemini, its flagship suite of generative AI models, apps, and services. But what\u2019s Gemini? How can you use it? And how does it stack up to other generative AI tools such as OpenAI\u2019s ChatGPT, Meta\u2019s Llama, and Microsoft\u2019s Copilot?\nTo make it easier to keep up with the latest Gemini developments, we\u2019ve put together this handy guide, which we\u2019ll keep updated as new Gemini models, features, and news about Google\u2019s plans for Gemini are released.\nWhat is Gemini?\nGemini is Google\u2019s long-promised, next-gen generative AI model family. Developed by Google\u2019s AI research labs DeepMind and Google Research, it comes in four flavors:\nGemini Ultra\nGemini Pro\nGemini Flash , a speedier, \u201cdistilled\u201d version of Pro. It also comes in a slightly smaller and faster version, called Gemini Flash-8B.\n, a speedier, \u201cdistilled\u201d version of Pro. It also comes in a slightly smaller and faster version, called Gemini Flash-8B. Gemini Nano, two small models: Nano-1 and the slightly more capable Nano-2, which is meant to run offline\nAll Gemini models were trained to be natively multimodal \u2014 that is, able to work with and analyze more than just text. Google says they were pre-trained and fine-tuned on a variety of public, proprietary, and licensed audio, images, and videos; a set of codebases; and text in different languages.\nThis sets Gemini apart from models such as Google\u2019s own LaMDA, which was trained exclusively on text data. LaMDA can\u2019t understand or generate anything beyond text (e.g., essays, emails, and so on), but that isn\u2019t necessarily the case with Gemini models.\nWe\u2019ll note here that the ethics and legality of training models on public data, in some cases without the data owners\u2019 knowledge or consent, are murky. Google has an AI indemnification policy to shield certain Google Cloud customers from lawsuits should they face them, but this policy contains carve-outs. Proceed with caution \u2014 particularly if you\u2019re intending on using Gemini commercially.\nWhat\u2019s the difference between the Gemini apps and Gemini models?\nGemini is separate and distinct from the Gemini apps on the web and mobile (formerly Bard).\nThe Gemini apps are clients that connect to various Gemini models and layer a chatbot-like interface on top. Think of them as front ends for Google\u2019s generative AI, analogous to ChatGPT and Anthropic\u2019s Claude family of apps.\nImage Credits:Google\nGemini on the web lives here. On Android, the Gemini app replaces the existing Google Assistant app. And on iOS, the Google and Google Search apps serve as that platform\u2019s Gemini clients.\nOn Android, it also recently became possible to bring up the Gemini overlay on top of any app to ask questions about what\u2019s on the screen (e.g., a YouTube video). Just press and hold a supported smartphone\u2019s power button or say, \u201cHey Google\u201d; you\u2019ll see the overlay pop up.\nGemini apps can accept images as well as voice commands and text \u2014 including files like PDFs and soon videos, either uploaded or imported from Google Drive \u2014 and generate images. As you\u2019d expect, conversations with Gemini apps on mobile carry over to Gemini on the web and vice versa if you\u2019re signed in to the same Google Account in both places.\nGemini Advanced\nThe Gemini apps aren\u2019t the only means of recruiting Gemini models\u2019 assistance with tasks. Slowly but surely, Gemini-imbued features are making their way into staple Google apps and services like Gmail and Google Docs.\nTo take advantage of most of these, you\u2019ll need the Google One AI Premium Plan. Technically a part of Google One, the AI Premium Plan costs $20 and provides access to Gemini in Google Workspace apps like Docs, Maps, Slides, Sheets, Drive, and Meet. It also enables what Google calls Gemini Advanced, which brings the company\u2019s more sophisticated Gemini models to the Gemini apps.\nGemini Advanced users get extras here and there, too, like priority access to new features, the ability to run and edit Python code directly in Gemini, and a larger \u201ccontext window.\u201d Gemini Advanced can remember the content of \u2014 and reason across \u2014 roughly 750,000 words in a conversation (or 1,500 pages of documents). That\u2019s compared to the 24,000 words (or 48 pages) the vanilla Gemini app can handle.\nImage Credits:Google\nGemini Advanced also gives users access to Google\u2019s new Deep Research feature, which uses \u201cadvanced reasoning\u201d and \u201clong context capabilities\u201d to generate research briefs. After you prompt the chatbot, it creates a multi-step research plan, asks you to approve it, and then Gemini takes a few minutes to search the web and generate an extensive report based on your query. It\u2019s meant to answer more complex questions such as, \u201cCan you help me redesign my kitchen?\u201d\nGoogle also offers Gemini Advanced users a memory feature, that allows the chatbot to use your old conversations with Gemini as context for your current conversation.\nAnother Gemini Advanced exclusive is trip planning in Google Search, which creates custom travel itineraries from prompts. Taking into account things like flight times (from emails in a user\u2019s Gmail inbox), meal preferences, and information about local attractions (from Google Search and Maps data), as well as the distances between those attractions, Gemini will generate an itinerary that updates automatically to reflect any changes.\nGemini across Google services is also available to corporate customers through two plans, Gemini Business (an add-on for Google Workspace) and Gemini Enterprise. Gemini Business costs as low as $6 per user per month, while Gemini Enterprise \u2014 which adds meeting note-taking and translated captions as well as document classification and labeling \u2014 is generally more expensive, but is priced based on a business\u2019s needs. (Both plans require an annual commitment.)\nIn Gmail, Gemini lives in a side panel that can write emails and summarize message threads. You\u2019ll find the same panel in Docs, where it helps you write and refine your content and brainstorm new ideas. Gemini in Slides generates slides and custom images. And Gemini in Google Sheets tracks and organizes data, creating tables and formulas.\nGoogle\u2019s AI chatbot recently came to Maps, where Gemini can summarize reviews about coffee shops or offer recommendations about how to spend a day visiting a foreign city.\nGemini\u2019s reach extends to Drive as well, where it can summarize files and folders and give quick facts about a project. In Meet, meanwhile, Gemini translates captions into additional languages.\nImage Credits:Google\nGemini recently came to Google\u2019s Chrome browser in the form of an AI writing tool. You can use it to write something completely new or rewrite existing text; Google says it\u2019ll consider the web page you\u2019re on to make recommendations.\nElsewhere, you\u2019ll find hints of Gemini in Google\u2019s database products, cloud security tools, and app development platforms (including Firebase and Project IDX), as well as in apps like Google Photos (where Gemini handles natural language search queries), YouTube (where it helps brainstorm video ideas), and the NotebookLM note-taking assistant.\nCode Assist (formerly Duet AI for Developers), Google\u2019s suite of AI-powered assistance tools for code completion and generation, is offloading heavy computational lifting to Gemini. So are Google\u2019s security products underpinned by Gemini, like Gemini in Threat Intelligence, which can analyze large portions of potentially malicious code and let users perform natural language searches for ongoing threats or indicators of compromise.\nGemini extensions and Gems\nAnnounced at Google I/O 2024, Gemini Advanced users can create Gems, custom chatbots powered by Gemini models. Gems can be generated from natural language descriptions \u2014 for example, \u201cYou\u2019re my running coach. Give me a daily running plan\u201d \u2014 and shared with others or kept private.\nGems are available on desktop and mobile in 150 countries and most languages. Eventually, they\u2019ll be able to tap an expanded set of integrations with Google services, including Google Calendar, Tasks, Keep, and YouTube Music, to complete custom tasks.\nImage Credits:Google\nSpeaking of integrations, the Gemini apps on the web and mobile can tap into Google services via what Google calls \u201cGemini extensions.\u201d Gemini today integrates with Google Drive, Gmail, and YouTube to respond to queries such as \u201cCould you summarize my last three emails?\u201d Later this year, Gemini will be able to take additional actions with Google Calendar, Keep, Tasks, YouTube Music and Utilities, the Android-exclusive apps that control on-device features like timers and alarms, media controls, the flashlight, volume, Wi-Fi, Bluetooth, and so on.\nGemini Live in-depth voice chats\nAn experience called Gemini Live allows users to have \u201cin-depth\u201d voice chats with Gemini. It\u2019s available in the Gemini apps on mobile and the Pixel Buds Pro 2, where it can be accessed even when your phone\u2019s locked.\nWith Gemini Live enabled, you can interrupt Gemini while the chatbot\u2019s speaking (in one of several new voices) to ask a clarifying question, and it\u2019ll adapt to your speech patterns in real time. At some point, Gemini is supposed to gain visual understanding, allowing it to see and respond to your surroundings, either via photos or video captured by your smartphones\u2019 cameras.\nImage Credits:Google\nLive is also designed to serve as a virtual coach of sorts, helping you rehearse for events, brainstorm ideas, and so on. For instance, Live can suggest which skills to highlight in an upcoming job or internship interview, and it can give public speaking advice.\nYou can read our review of Gemini Live here. Spoiler alert: We think the feature has a ways to go before it\u2019s super useful \u2014 but it\u2019s early days, admittedly.\nImage generation via Imagen 3\nGemini users can generate artwork and images using Google\u2019s built-in Imagen 3 model.\nGoogle says that Imagen 3 can more accurately understand the text prompts that it translates into images versus its predecessor, Imagen 2, and is more \u201ccreative and detailed\u201d in its generations. In addition, the model produces fewer artifacts and visual errors (at least according to Google), and is the best Imagen model yet for rendering text.\nA sample from Imagen 3. Image Credits:Google\nBack in February, Google was forced to pause Gemini\u2019s ability to generate images of people after users complained of historical inaccuracies. But in August, the company reintroduced people generation for certain users, specifically English-language users signed up for one of Google\u2019s paid Gemini plans (e.g., Gemini Advanced) as part of a pilot program.\nGemini for teens\nIn June, Google introduced a teen-focused Gemini experience, allowing students to sign up via their Google Workspace for Education school accounts.\nThe teen-focused Gemini has \u201cadditional policies and safeguards,\u201d including a tailored onboarding process and an \u201cAI literacy guide\u201d to (as Google phrases it) \u201chelp teens use AI responsibly.\u201d Otherwise, it\u2019s nearly identical to the standard Gemini experience, down to the \u201cdouble check\u201d feature that looks across the web to see if Gemini\u2019s responses are accurate.\nGemini in smart home devices\nA growing number of Google-made devices tap Gemini for enhanced functionality, from the Google TV Streamer to the Pixel 9 and 9 Pro to the newest Nest Learning Thermostat.\nOn the Google TV Streamer, Gemini uses your preferences to curate content suggestions across your subscriptions and summarize reviews and even whole seasons of TV.\nImage Credits:Google\nOn the latest Nest thermostat (as well as Nest speakers, cameras, and smart displays), Gemini will soon bolster Google Assistant\u2019s conversational and analytic capabilities.\nSubscribers to Google\u2019s Nest Aware plan later this year will get a preview of new Gemini-powered experiences like AI descriptions for Nest camera footage, natural language video search and recommended automations. Nest cameras will understand what\u2019s happening in real-time video feeds (e.g., when a dog\u2019s digging in the garden), while the companion Google Home app will surface videos and create device automations given a description (e.g., \u201cDid the kids leave their bikes in the driveway?,\u201d \u201cHave my Nest thermostat turn on the heating when I get home from work every Tuesday\u201d).\nGemini will soon be able to summarize security camera footage from Nest devices. Image Credits:Google\nAlso later this year, Google Assistant will get a few upgrades on Nest-branded and other smart home devices to make conversations feel more natural. Improved voices are on the way, in addition to the ability to ask follow-up questions and \u201c[more] easily go back and forth.\u201d\nWhat can the Gemini models do?\nBecause Gemini models are multimodal, they can perform a range of multimodal tasks, from transcribing speech to captioning images and videos in real time. Many of these capabilities have reached the product stage (as alluded to in the previous section), and Google is promising much more in the not-too-distant future.\nOf course, it\u2019s a bit hard to take the company at its word. Google seriously underdelivered with the original Bard launch. More recently, it ruffled feathers with a video purporting to show Gemini\u2019s capabilities that was more or less aspirational \u2014 not live.\nAlso, Google offers no fix for some of the underlying problems with generative AI tech today, like its encoded biases and tendency to make things up (i.e., hallucinate). Neither do its rivals, but it\u2019s something to keep in mind when considering using or paying for Gemini.\nAssuming for the purposes of this article that Google is being truthful with its recent claims, here\u2019s what the different tiers of Gemini can do now and what they\u2019ll be able to do once they reach their full potential:\nWhat you can do with Gemini Ultra\nGoogle says that Gemini Ultra \u2014 thanks to its multimodality \u2014 can be used to help with things like physics homework, solving problems step-by-step on a worksheet, and pointing out possible mistakes in already filled-in answers.\nUltra can also be applied to tasks such as identifying scientific papers relevant to a problem, Google says. The model can extract information from several papers, for instance, and update a chart from one by generating the formulas necessary to re-create the chart with more timely data.\nGemini Ultra technically supports image generation. But that capability hasn\u2019t made its way into the productized version of the model yet \u2014 perhaps because the mechanism is more complex than how apps such as ChatGPT generate images. Rather than feed prompts to an image generator (like DALL-E 3, in ChatGPT\u2019s case), Gemini outputs images \u201cnatively,\u201d without an intermediary step.\nUltra is available as an API through Vertex AI, Google\u2019s fully managed AI dev platform, and AI Studio, Google\u2019s web-based tool for app and platform developers.\nGemini Pro\u2019s capabilities\nGoogle says that Gemini Pro is an improvement over LaMDA in its reasoning, planning, and understanding capabilities. The latest version, Gemini 1.5 Pro \u2014 which powers the Gemini apps for Gemini Advanced subscribers \u2014 exceeds even Ultra\u2019s performance in some areas.\nGemini 1.5 Pro is improved in a number of areas compared with its predecessor, Gemini 1.0 Pro, perhaps most obviously in the amount of data that it can process. Gemini 1.5 Pro can take in up to 1.4 million words, two hours of video, or 22 hours of audio and can reason across or answer questions about that data (more or less).\nGemini 1.5 Pro became generally available on Vertex AI and AI Studio in June alongside a feature called code execution, which aims to reduce bugs in code that the model generates by iteratively refining that code over several steps. (Code execution also supports Gemini Flash.)\nWithin Vertex AI, developers can customize Gemini Pro to specific contexts and use cases via a fine-tuning or \u201cgrounding\u201d process. For example, Pro (along with other Gemini models) can be instructed to use data from third-party providers like Moody\u2019s, Thomson Reuters, ZoomInfo and MSCI, or source information from corporate datasets or Google Search instead of its wider knowledge bank. Gemini Pro can also be connected to external, third-party APIs to perform particular actions, like automating a back-office workflow.\nAI Studio offers templates for creating structured chat prompts with Pro. Developers can control the model\u2019s creative range and provide examples to give tone and style instructions \u2014 and also tune Pro\u2019s safety settings.\nVertex AI Agent Builder lets people build Gemini-powered \u201cagents\u201d within Vertex AI. For example, a company could create an agent that analyzes previous marketing campaigns to understand a brand style and then apply that knowledge to help generate new ideas consistent with the style.\nGemini Flash is lighter but packs a punch\nWhile the first version of Gemini Flash was made for less demanding workloads, the newest version, 2.0 Flash, is now Google\u2019s flagship AI model. Google calls Gemini 2.0 Flash its AI model for the agentic era. The model can natively generate images and audio, in addition to text, and can use tools like Google Search and interact with external APIs.\nThe 2.0 Flash model is faster than Gemini\u2019s previous generation of models and even outperforms some of the larger Gemini 1.5 models on benchmarks measuring coding and image analysis. You can try an experimental version of 2.0 Flash in the web version of Gemini or through Google\u2019s AI developer platforms, and a production version of the model should land in January.\nAn offshoot of Gemini Pro that\u2019s small and efficient, built for narrow, high-frequency generative AI workloads, Flash is multimodal like Gemini Pro, meaning it can analyze audio, video, images, and text (but it can only generate text). Google says that Flash is particularly well-suited for tasks like summarization and chat apps, plus image and video captioning and data extraction from long documents and tables.\nDevs using Flash and Pro can optionally leverage context caching, which lets them store large amounts of information (e.g., a knowledge base or database of research papers) in a cache that Gemini models can quickly and relatively cheaply access. Context caching is an additional fee on top of other Gemini model usage fees, however.\nGemini Nano can run on your phone\nGemini Nano is a much smaller version of the Gemini Pro and Ultra models, and it\u2019s efficient enough to run directly on (some) devices instead of sending the task to a server somewhere. So far, Nano powers a couple of features on the Pixel 8 Pro, Pixel 8, Pixel 9 Pro, Pixel 9 and Samsung Galaxy S24, including Summarize in Recorder and Smart Reply in Gboard.\nThe Recorder app, which lets users push a button to record and transcribe audio, includes a Gemini-powered summary of recorded conversations, interviews, presentations, and other audio snippets. Users get summaries even if they don\u2019t have a signal or Wi-Fi connection \u2014 and in a nod to privacy, no data leaves their phone in process.\nImage Credits:Google\nNano is also in Gboard, Google\u2019s keyboard replacement. There, it powers a feature called Smart Reply, which helps to suggest the next thing you\u2019ll want to say when having a conversation in a messaging app such as WhatsApp.\nIn the Google Messages app on supported devices, Nano drives Magic Compose, which can craft messages in styles like \u201cexcited,\u201d \u201cformal,\u201d and \u201clyrical.\u201d\nGoogle says that a future version of Android will tap Nano to alert users to potential scams during calls. The new weather app on Pixel phones uses Gemini Nano to generate tailored weather reports. And TalkBack, Google\u2019s accessibility service, employs Nano to create aural descriptions of objects for low-vision and blind users.\nHow much do the Gemini models cost?\nGemini 1.0 Pro (the first version of Gemini Pro), 1.5 Pro, and Flash are available through Google\u2019s Gemini API for building apps and services \u2014 all with free options. But the free options impose usage limits and leave out certain features, like context caching and batching.\nGemini models are otherwise pay-as-you-go. Here\u2019s the base pricing \u2014 not including add-ons like context caching \u2014 as of September 2024:\nGemini 1.0 Pro: 50 cents per 1 million input tokens, $1.50 per 1 million output tokens\n50 cents per 1 million input tokens, $1.50 per 1 million output tokens Gemini 1.5 Pro: $1.25 per 1 million input tokens (for prompts up to 128K tokens) or $2.50 per 1 million input tokens (for prompts longer than 128K tokens); $5 per 1 million output tokens (for prompts up to 128K tokens) or $10 per 1 million output tokens (for prompts longer than 128K tokens)\n$1.25 per 1 million input tokens (for prompts up to 128K tokens) or $2.50 per 1 million input tokens (for prompts longer than 128K tokens); $5 per 1 million output tokens (for prompts up to 128K tokens) or $10 per 1 million output tokens (for prompts longer than 128K tokens) Gemini 1.5 Flash: 7.5 cents per 1 million input tokens (for prompts up to 128K tokens), 15 cents per 1 million input tokens (for prompts longer than 128K tokens), 30 cents per 1 million output tokens (for prompts up to 128K tokens), 60 cents per 1 million output tokens (for prompts longer than 128K tokens)\n7.5 cents per 1 million input tokens (for prompts up to 128K tokens), 15 cents per 1 million input tokens (for prompts longer than 128K tokens), 30 cents per 1 million output tokens (for prompts up to 128K tokens), 60 cents per 1 million output tokens (for prompts longer than 128K tokens) Gemini 1.5 Flash-8B: 3.75 cents per 1 million input tokens (for prompts up to 128K tokens), 7.5 cents per 1 million input tokens (for prompts longer than 128K tokens), 15 cents per 1 million output tokens (for prompts up to 128K tokens), 30 cents per 1 million output tokens (for prompts longer than 128K tokens)\nTokens are subdivided bits of raw data, like the syllables \u201cfan,\u201d \u201ctas,\u201d and \u201ctic\u201d in the word \u201cfantastic\u201d; 1 million tokens is equivalent to about 700,000 words. Input refers to tokens fed into the model, while output refers to tokens that the model generates.\nUltra and 2.0 Flash pricing has yet to be announced, and Nano is still in early access.\nWhat\u2019s the latest on Project Astra?\nProject Astra is Google DeepMind\u2019s effort to create AI-powered apps and \u201cagents\u201d for real-time, multimodal understanding. In demos, Google has shown how the AI model can simultaneously process live video and audio. Google released an app version of Project Astra to a small number of trusted testers in December but has no plans for a broader release right now.\nThe company would like to put Project Astra in a pair of smart glasses. Google also gave a prototype of some glasses with Project Astra and augmented reality capabilities to a few trusted testers in December. However, there\u2019s not a clear product at this time, and it\u2019s unclear when Google would actually release something like this.\nProject Astra is still just that, a project, and not a product. However, the demos of Astra reveal what Google would like its AI products to do in the future.\nIs Gemini coming to the iPhone?\nIt might.\nApple has said that it\u2019s in talks to put Gemini and other third-party models to use for a number of features in its Apple Intelligence suite. Following a keynote presentation at WWDC 2024, Apple SVP Craig Federighi confirmed plans to work with models, including Gemini, but he didn\u2019t divulge any additional details.\nThis post was originally published February 16, 2024, and has since been updated to include new information about Gemini and Google\u2019s plans for it.\n\ntitle2:\nAdobe shares plunge 14% on disappointing 2025 revenue guidance\ndescription2:\nAdobe CEO Shantanu Narayen speaks during an interview with CNBC on the floor of the New York Stock Exchange on Feb. 20, 2024.\nAdobe shares fell 14% on Thursday, their steepest drop since September 2022, after the software vendor issued disappointing revenue guidance.\nSales in the fiscal first quarter will be between $5.63 billion and $5.68 billion, Adobe said in its fourth-quarter earnings report late Wednesday. Analysts on average were expecting revenue of $5.73 billion, according to LSEG.\nAnalysts at TD Cowen downgraded the stock to hold from buy, while Wells Fargo kept its buy rating following what it called a \"frustrating '24\" for the company. The stock is now down 20% for the year, badly trailing the Nasdaq, which is up 33% and crossed the 20,000 mark for the first time on Wednesday.\nWhile Adobe's forecast trailed estimates, the company's fourth-quarter results exceeded expectations.\nAdjusted earnings per share came in at $4.81, topping the average analyst estimate of $4.66, according to LSEG. Revenue in the fourth quarter increased 11% to $5.61 billion, beating the average estimate of $5.54 billion.\nMonetizing generative artificial intelligence, especially in stand-alone offerings such as Firefly image generation or additional offerings across the Creative Cloud, has been central to Adobe's growth strategy.\nAnalysts at Deutsche Bank maintained their buy rating but lowered their target price from $650 to $600.\n\"These results and guidance require a bit of faith in the full year next year,\" the analysts wrote. Still, they said, \"We see tangible evidence that Adobe is one of few application software companies in our coverage successfully monetizing generative AI today.\"\n\ntitle3:\nOpenAI says ChatGPT has recovered after hours-long outage\ndescription3:\nOpenAI said in an early Thursday X post that its popular ChatGPT assistant, Sora video generator and programming interface for software developers were working again after hours of downtime.\nChatGPT has hit the mainstream. Sam Altman, OpenAI's CEO, said on December 4 that the company's technology was reaching 300 million active users each week. On Wednesday, Apple released new versions of its software for the iPhone, iPad and Mac that bring integrations with ChatGPT.\nAccording to an OpenAI status page, ChatGPT was down for just over four hours. An outage in June lasted for over five hours.\nOpenAI was valued at $157 billion in a funding round in October that included participation from existing backer Microsoft as well as chipmaker Nvidia . The company's rapid ascent began with the launch of ChatGPT in late 2022 and has been the biggest story in the tech industry over the last couple years.\nOn Monday OpenAI said it was releasing Sora to people in the U.S. and most other countries, but on Tuesday, Altman wrote on X that \"we significantly underestimated demand for sora; it is going to take awhile to get everyone access.\"\nWATCH: iPhone's ChatGPT upgrade: Here's what to know\n\ntitle4:\nMeta debuts a tool for watermarking AI-generated videos\ndescription4:\nThrow a stone and you\u2019ll likely hit a deepfake. The commoditization of generative AI has led to an absolute explosion of fake content online: According to ID verification platform Sumsub, there\u2019s been a 4x increase in deepfakes worldwide from 2023 to 2024. In 2024, deepfakes accounted for 7% of all fraud, per Sumsub, ranging from impersonations and account takeovers to sophisticated social engineering campaigns.\nIn what it hopes will be a meaningful contribution to the fight against deepfakes, Meta is releasing a tool to apply imperceptible watermarks to AI-generated video clips. Announced on Thursday, the tool, called Meta Video Seal, is available in open source and designed to be integrated into existing software. The tool joins Meta\u2019s other watermarking tools, Watermark Anything (re-released today under a permissive license) and Audio Seal.\n\u201cWe developed Video Seal to provide a more effective video watermarking solution, particularly for detecting AI-generated videos and protecting originality,\u201d Pierre Fernandez, AI research scientist at Meta, told TechCrunch in an interview.\nVideo Seal isn\u2019t the first technology of its kind. DeepMind\u2019s SynthID can watermark videos, and Microsoft has its own video watermarking methodologies.\nBut Fernandez asserts that many existing approaches fall short.\n\u201cWhile other watermarking tools exist, they don\u2019t offer sufficient robustness to video compression, which is very prevalent when sharing content through social platforms; weren\u2019t efficient enough to run at scale; weren\u2019t open or reproducible; or were derived from image watermarking, which is suboptimal for videos,\u201d Fernandez said.\nIn addition to a watermark, Video Seal can add a hidden message to videos that can later be uncovered to determine their origins. Meta claims that Video Seal is resilient against common edits like blurring and cropping, as well as popular compression algorithms.\nFernandez admits that Video Seal has certain limitations, mainly the trade-off between how perceptible the tool\u2019s watermarks are and their overall resilience to manipulation. Heavy compression and significant edits may alter the watermarks or render them unrecoverable, he added.\nOf course, the bigger problem facing Video Seal is that devs and industry won\u2019t have much reason to adopt it, particularly those already using proprietary solutions. In a bid to address that, Meta is launching a public leaderboard, Meta Omni Seal Bench, dedicated to comparing the performance of various watermarking methods, and organizing a workshop on watermarking this year at ICLR, a major AI conference.\n\u201cWe hope that more and more AI researchers and developers will integrate some form of watermarking into their work,\u201d Fernandez said. \u201cWe want to collaborate with the industry and the academic community to progress faster in the field.\u201d\"\n\nResponse:",
    "Script": "i'm wall-e, and this is your ai briefing for friday, december 13th.\n\nfirst up, microsoft introduces its latest generative ai model, phi-4, in a research preview. unveiled on the azure ai foundry platform, phi-4 is a 14-billion-parameter model known for its enhanced math problem-solving skills. microsoft attributes its improvements to high-quality synthetic datasets and advanced post-training processes, allowing it to compete with models like gpt-4o mini and claude 3.5 haiku.\n\nturning to google, their generative ai suite, gemini, is expanding its capabilities. developed by deepmind and google research, gemini models are natively multimodal, handling text, audio, images, and video. google is pushing these models further, integrating them into existing apps like gmail and google docs via its ai premium plan. the launch of gemini business and enterprise plans indicates a strategic move to bolster ai infrastructure across corporate settings.\n\nin market news, adobe shares tumbled 14% following disappointing 2025 revenue guidance. despite beating q4 earnings expectations, with adjusted eps at $4.81, revenue predictions for the upcoming fiscal quarter fell short. analysts are divided: td cowen downgraded adobe to hold, while wells fargo remains optimistic, citing the company\u2019s success in monetizing generative ai.\n\nopenai's chatgpt service has bounced back after a significant outage. with 300 million weekly users, the downtime lasted over four hours, affecting related services like the sora video generator. the outage underscores the fragile infrastructure supporting ai's rapid integration into consumer tech, notably as apple releases new software letting devices integrate with chatgpt.\n\nfinally, meta launches video seal to combat the rise of deepfakes, which surged 4x year-over-year. the tool offers robust video watermarking, claiming resilience against video compression. meta is pushing for wider adoption by launching a leaderboard to assess watermarking techniques, aiming for collaboration across academia and industry to address ai-generated content's transparency.\n\nthat's all for today. we'll see you back here tomorrow",
    "Polished Script": "\"i'm wall-e, welcoming you to today's tech briefing for friday, december 13th.\n\nmicrosoft is in the headlines with the introduction of its newest generative ai model, phi-4, in a research preview. this 14-billion-parameter powerhouse is making waves on the azure ai foundry platform with its advanced math problem-solving capabilities. microsoft credits these advancements to high-quality synthetic datasets and cutting-edge post-training processes, positioning phi-4 to compete with models like gpt-4o mini and claude 3.5 haiku.\n\nturning to google, the tech giant's generative ai suite, gemini, is expanding its reach. developed in partnership with deepmind and google research, these natively multimodal models handle text, audio, images, and video seamlessly. by integrating gemini into apps like gmail and google docs via its ai premium plan, google aims to further entrench itself in the ai landscape. the rollout of gemini's business and enterprise plans underscores a strategic push to enhance ai infrastructure across corporate environments.\n\nin market movements, adobe's shares took a hit, dropping 14% following lackluster 2025 revenue guidance. despite beating q4 earnings expectations with adjusted eps at $4.81, upcoming fiscal quarter revenue forecasts fell short. analysts are split: td cowen downgraded adobe to 'hold,' whereas wells fargo remains optimistic, highlighting adobe's success in leveraging generative ai.\n\nmeanwhile, openai's chatgpt service has recovered after a significant outage. with 300 million weekly users affected by the more than four-hour downtime, the incident also impacted services like the sora video generator. this highlights the fragile infrastructure underpinning the rapid consumer tech integration of ai, especially as apple rolls out new software enabling device integration with chatgpt.\n\nlastly, meta is tackling the rise of deepfakes, which have surged fourfold year-over-year, by launching video seal. this tool provides robust video watermarking, claiming resilience against video compression. to promote wider adoption, meta has introduced a leaderboard for evaluating watermarking techniques, aiming to foster collaboration across academia and industry to ensure transparency in ai-generated content.\n\nthat's all for today. we'll see you back here tomorrow.\"",
    "Podcast Title": "EP-172 Microsoft's Ai Leap with Phi-4 \ud83d\udca1, Google's Gemini Expansion \ud83c\udf0c, Adobe's Stock Shake-up \ud83d\udcc9",
    "Podcast Description": "```html\n<p>i'm wall-e, welcoming you to today's tech briefing for friday, december 13th. explore the key tech stories making headlines:</p>\n<ul>\n<li><strong>microsoft's phi-4 ai model:</strong> microsoft unveils its 14-billion-parameter phi-4 model on azure ai foundry, excelling in math problem-solving. enhanced by synthetic datasets and post-training processes, it's set to rival gpt-4o mini and claude 3.5 haiku.</li>\n<li><strong>google's gemini ai suite:</strong> collaboration with deepmind and google research yields the multimodal gemini models. integrated into apps like gmail via ai premium plan, google aims to strengthen its ai footprint, rolling out business and enterprise plans.</li>\n<li><strong>adobe's market outlook:</strong> adobe shares drop 14% amid weak 2025 revenue guidance. despite q4 earnings beat, fiscal forecasts disappoint. opinions split as td cowen downgrades, while wells fargo remains positive citing ai leverage.</li>\n<li><strong>openai's chatgpt recovery:</strong> chatgpt service back on track after a major outage affecting 300 million weekly users and related services like sora video generator, highlighting the need for robust tech infrastructure.</li>\n<li><strong>meta's fight against deepfakes:</strong> surge in deepfakes prompts meta to introduce video seal, a resilient video watermarking tool. meta pushes for transparency with a new watermarking leaderboard encouraging academic and industry collaboration.</li>\n</ul>\n<p>that's all for today. we'll see you back here tomorrow.</p>\n```",
    "Image Prompt": "the podcast cover image presents a futuristic landscape where advanced technologies merge in a coherent visual symphony. in the foreground, a sleek, ethereal figure embodies ai, radiating a glowing, fractal network pattern that represents microsoft's phi-4. this figure seamlessly merges into the center of the image, where a vast digital universe of galaxies and stars expands infinitely, symbolizing google's gemini expansion. these cosmic elements swirl gracefully around the ai figure, creating a sense of boundless exploration and innovation. nearby, scattered creative tools like digital paintbrushes and cameras subtly float, hinting at adobe's transformative influence in the tech realm. the background features a dynamic, fluctuating graph subtly interweaved with circuit board patterns, illustrating adobe's stock market movements. this harmonious composition captures the essence of technological progress and interconnectedness, enveloping the viewer in an aura of futuristic possibilities"
}